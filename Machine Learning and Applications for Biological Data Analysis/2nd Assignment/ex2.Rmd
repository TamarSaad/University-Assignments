---
title: "Targil 2"
author: "Tamar Saad 207256991 & Or Arbel 207016098 & Rachel Weinberger 208812628"
date: "5/19/2022"
abstract: In this assignment we took a table containing data about different types of seeds, and we wanted to use the data in order to create a machine learning model that will classify seeds by their type.
output: pdf_document
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Libraries uploading:
```{r workspace, results='hide', message=FALSE, warning=FALSE}
library(plyr)
library(dplyr)
library(stringr)
library(gplots)
library(ggplot2)
library(forcats)
library(data.table)
library(reshape2)
library(affycoretools)
library(factoextra)
library(ggvis)
library(class)
library(gmodels)
library(plotly)
library(C50)

```

Upload and examine the data
```{r}
seeds<-read.csv("seeds.csv")
summary(seeds)
```

After examining the summary table, it seems there is no need in cleaning the data or normalizing one of the features. All the features are numeric, and all are in a standard range.

Before we create a classification model using machine learning tools, we want to see if there is a distinct difference between the seed types. If the given data doesn't reflect the difference between the seeds types, maybe there will be no use in creating a classification model. In order to examine it, we create a PCA plot.
In addition, we want to look at the contribution of each feature to the PCA components, so we can see what features are the most significant.
```{r}
pca <- prcomp(seeds)
pca_to_show <- data.frame(
  PC1 = pca$x[, 1],
  PC2 = pca$x[, 2],
  classification = as.factor(seeds$Type)
)

ggplot(pca_to_show, aes(x = PC1, y = PC2, col = classification)) +
  geom_point()

fviz_pca_var(pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )


```
As we can see, the different types of seeds cluster separately, so the features that we have can be used to create a classification model.
Moreover, we can see that the features that contribute to the variability the most are the 'Area' feature, the 'Asymmetry Coeff' and the 'Perimeter'. So, if we will have trouble with getting good results from the classification models, perhaps we could remove some of our less contributing features in order to reduce background noise.



Classification models:

We chose to use two different machine learning models in order to classify the data: KNN and Decision Tree.



KNN:


We wanted to see the distribution of the types in the data, to see if we have enough samples from each type.
```{r}
seeds$Type<-as.factor(seeds$Type)
table(seeds$Type)
```

Normalize the data by MIN/MAX:

```{r}
# create the min/max normalization function:
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

#normalize the data without the type column(8)
seeds_normalize <- as.data.frame(lapply(seeds[1:7], normalize))

str(seeds_normalize)
```

In the beginning we shuffle the data and split it to train (1) and test (2) by creating an index

```{r}
# create an index with the desired proportions
set.seed(123)
ind <- sample(2, nrow(seeds), replace=TRUE, prob=c(0.8, 0.2))
```

Create a training set and a test set:

```{r}
# training set
seeds.training_n <- seeds_normalize[ind==1, 1:7]

# Compose training labels
seeds.trainLabels <- seeds[ind==1,8]

# test set
seeds.test_n <- seeds_normalize[ind==2, 1:7]

# Compose test labels
seeds.testLabels <- seeds[ind==2, 8]

# check if we got about 30% from each seed type
prop.table(table(seeds.testLabels))
prop.table(table(seeds.trainLabels))
```

Build 3 models with different values of k: 3, 9, and 15

```{r}

seeds_pred_k3 <- knn(train = seeds.training_n, test = seeds.test_n,
                     cl = seeds.trainLabels, k=3)
seeds_pred_k9 <- knn(train = seeds.training_n, test = seeds.test_n,
                     cl = seeds.trainLabels, k=9)
seeds_pred_k15 <- knn(train = seeds.training_n, test = seeds.test_n,
                      cl = seeds.trainLabels, k=15)

```

Let's Examine the results of the different models:

```{r}

CrossTable(x = seeds.testLabels, y = seeds_pred_k3, prop.chisq=FALSE)
CrossTable(x = seeds.testLabels, y = seeds_pred_k9, prop.chisq=FALSE)
CrossTable(x = seeds.testLabels, y = seeds_pred_k15, prop.chisq=FALSE)
```

We tried to do z-score normalization too, we're doing the same steps as in min-max normalization:
 
```{r}
# z-score normalization
seeds_z <- as.data.frame(scale(seeds[-8]))

seeds.train_z <- seeds_z[ind==1, 1:7]
seeds.test_z <- seeds_z[ind==2,1:7 ]
seeds.trainLabels_z <- seeds[ind==1, 8]
seeds.testLabels_z <- seeds[ind==2, 8]
```

We got the best results for k=3 in min-max normalization so we will check the same k in z-score normalization:

```{r}
seeds_test_pred_z_k3 <- knn(train = seeds.train_z, test = seeds.test_z, cl = 
                              seeds.trainLabels_z, k=3)
CrossTable(x = seeds.testLabels_z, y = seeds_test_pred_z_k3, prop.chisq=FALSE)

```
When we tried to use the z-score normalization further we saw that it led us to over-fitting, so we decided to continue with min-max normalization.
 
Plots of the classification results by KNN algorithm:
We chose the axes to be the 'Area' and 'Asymmetry Coeff' because we saw earlier that these were the features that contributed to the variability the most.
 
```{r}
# prediction table
seeds.test_n_pred<-seeds.test_n
seeds.test_n_pred$Type<-seeds_pred_k3

# observed test
seeds.test_n$Type<-seeds.testLabels

# prediction
fig_p <- plot_ly() 
fig_p <- fig_p %>% add_trace(data=seeds.test_n_pred, x = ~Area, y = ~Asymmetry.Coeff, 
                             symbol = ~Type, split = ~Type, symbols = 
                               c('square-dot','circle-dot','diamond-dot'),
               type = 'scatter', mode = 'markers',  
               marker = list(size = 12, line = list(width = 1.5), color = 'lightyellow'))%>% 
  layout(title="Prediction")

# observed
fig_o <- plot_ly()
fig_o <- fig_o %>% add_trace(data=seeds.test_n, x = ~Area, y = ~Asymmetry.Coeff, symbol = 
                               ~Type, split = ~Type, symbols = c('square','circle','diamond'),
               type = 'scatter', mode = 'markers',  
               marker = list(size = 12, line = list(width = 1.5), color = 'lightyellow'))%>% 
  layout(title="Observed")


fig_p
fig_o

```

We can see that the plots are almost identical, which means that the model predicted almost all of the samples correctly.


As we can see, the KNN model gave us good classification results so we recommend to use it further on this data.
 
 
 
 
 Decision Tree:
 
 
Using Decision Tree (C5.0) algorithm to predict the seeds types. 
We used a hyper parameter called "minCases" that determines that 3 is the
smallest number of samples that must be put in at least two of the splits.
This hyper parameter turned to cause the algorithm to predict the seeds type
in the most accurate way. We tried different kinds of hyper parameters, and this one gave us the best results.
We used different groups of train and test than the ones we created in the KNN model, because it gave as better results.

```{r}
# sample 800 observations out of the total 1000
train_sample <- sample(199, 150)

# split into train/test
seeds_train <- seeds[train_sample, ]
seeds_test <- seeds[-train_sample, ]

# check that we got about 30% defaulted loans in each dataset:
prop.table(table(seeds_train$Type))
prop.table(table(seeds_test$Type))
```

Applying the model:

```{r}
# apply model in training data (8th column is the label to be predicted)
seeds_model <- C5.0(seeds_train[-8], seeds_train$Type, control = C5.0Control(minCases  = 3))

seeds_model
```
Plot and summary of the model:

```{r}
summary(seeds_model)
plot(seeds_model, main = 'Seeds Model')

```

We can see that the model is correct for about 98% of the samples, which means that the model is good.
To make sure that there isn't over fitting, we will predict the validation set, 
and see if the predictions are good for it too.

```{r}
# apply model on test data
seeds_pred <- predict(seeds_model, seeds_test)

CrossTable(seeds_test$Type, seeds_pred, prop.chisq = FALSE, prop.c = FALSE, 
           prop.r = FALSE, dnn = c('actual type', 'predicted type'))
```

We can see that the prediction was correct for most seeds, 
so it is possible to say that the model is good for this data-set.

In conclusion, we created here two classification models, one of KNN and one of Decision tree.
Both models gave us good results, and we can use both of them to classify our data.
We do think that for this data the KNN model gave better results, so this is the model we will prefer to use.
