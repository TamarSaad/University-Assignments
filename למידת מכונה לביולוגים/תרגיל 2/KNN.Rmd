---
title: "KNN"
author: "racheli"
date: "22 5 2022"
output:
  pdf_document: default
  html_document: default
---
KNN algorithm for seeds.csv file

```{r setup, include=FALSE}
library(ggplot2)
library(ggvis)
library(class)
library(gmodels)

seeds <- read.csv("seeds.csv", stringsAsFactors = FALSE)

seeds %>% ggvis(~Kernel.Length, ~Kernel.Width, fill = ~Type) %>% layer_points()

```

look at the data

```{r}
str(seeds)

```

We'll also looking at the variable we want to predict, TYPE:

```{r}
# look at Type distribution
table(seeds$Type)
```

normalize the data- MIN/MAX:

```{r}
# create the min/max normalization function:
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

#normalize the data without the type column(8)
seeds_normalize <- as.data.frame(lapply(seeds[1:7], normalize))

#check the normalize
summary(seeds_normalize$Type)

```

In the beginning we will shuffle the data and split it to train (1) and test (2) by create an index

```{r}
# create an index with the desired proportions
set.seed(123)
ind <- sample(2, nrow(seeds), replace=TRUE, prob=c(0.8, 0.2))
```

we create a training set and a test set:

```{r}
# training set
seeds.training_n <- seeds_normalize[ind==1, 1:7]

# Compose training labels
seeds.trainLabels <- seeds[ind==1,8]

# test set
seeds.test_n <- seeds_normalize[ind==2, 1:7]

# Compose test labels
seeds.testLabels <- seeds[ind==2, 8]

# Build the model
seeds_pred_k3 <- knn(train = seeds.training_n, test = seeds.test_n, cl = seeds.trainLabels, k=3)
seeds_pred_k9 <- knn(train = seeds.training_n, test = seeds.test_n, cl = seeds.trainLabels, k=9)
seeds_pred_k15 <- knn(train = seeds.training_n, test = seeds.test_n, cl = seeds.trainLabels, k=15)


CrossTable(x = seeds.testLabels, y = seeds_pred_k3, prop.chisq=FALSE)
CrossTable(x = seeds.testLabels, y = seeds_pred_k9, prop.chisq=FALSE)
CrossTable(x = seeds.testLabels, y = seeds_pred_k15, prop.chisq=FALSE)
```

 z-score normalization, we are doing the same steps as min-max normalization:
 
```{r}
# z-score normalization
seeds_z <- as.data.frame(scale(seeds[-8]))

seeds.train_z <- seeds_z[ind==1, 1:7]
seeds.test_z <- seeds_z[ind==2,1:7 ]
seeds.trainLabels_z <- seeds[ind==1, 8]
seeds.testLabels_z <- seeds[ind==2, 8]
```
we got the best results for k=3 in min-max normalization so we will check the same k in z-score normalization:
```{r}
seeds_test_pred_z_k3 <- knn(train = seeds.train_z, test = seeds.test_z, cl = seeds.trainLabels_z, k=3)
CrossTable(x = seeds.testLabels_z, y = seeds_test_pred_z_k3, prop.chisq=FALSE)

```
 The difference between min-max and z-score is not significant so we decide to use min-max normalization
 
 plot for the classification results by knn algorithm:
 
```{r}
# add type column
seeds.test_n$Type<-seeds.testLabels

library(plotly)

fig <- plot_ly() 
fig <- fig %>% add_trace(data=seeds.test_n, x = ~Kernel.Length, y = ~Kernel.Width, symbol = ~Type, split = ~Type, symbols = c('square-dot','circle-dot','diamond'),
               type = 'scatter', mode = 'markers',  
               marker = list(size = 12, line = list(width = 1.5), color = 'lightyellow'))%>% layout(title="Prediction Confidence on Test Split")

fig

```
 
 